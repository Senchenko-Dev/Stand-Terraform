- name: Major update of Pangolin
  block:

    - name: trigger revert check
      block:

        - name: touch trigger
          file:
            path: "{{ update_status_files.dir }}/.trigger_stop_update"
            state: touch
          become: true
          when: is_recovery_test_mode and trigger_stop05m

        - name: trigger stop updated
          include_role:
            name: common
            tasks_from: trigger_stop_update

        - name: print message monitoring trigger stop
          debug:
            msg: "{{ update_control_msgs.info.trigger_update_stop }}"

      when: inventory_hostname == 'master'

    - name: set major_main_migrate_db error
      set_fact:
        update_errors: "{{ update_errors|combine(data, recursive=True) }}"
        cacheable: yes
      vars:
        data:
          types:
            pg: 
              major_main_migrate_master_db: "{% if migrate_host == 'master' %}True{% else %}False{% endif %}"
              major_main_migrate_replica_db: "{% if migrate_host == 'replica' %}True{% else %}False{% endif %}"

    - name: set python interpretator
      set_fact:
        ansible_python_interpreter: '{{ python.postgresql_venv }}/bin/python3'

    - name: stop Pangolin on master host before started rsync
      block:

        - name: turn on pause mode after patroni nodes updated
          include_role:
            name: patroni
            tasks_from: update_with_patronictl.yml
          vars:
            change_params: "pause: true"

        - name: check exists pg service
          stat: path="{{ service_path_dir }}/postgresql.service"
          register: pg_service_exists

        - name: stop Pangolin
          service:
            name: postgresql
            state: stopped
          when: pg_service_exists.stat.exists

        - name: stop Pangolin
          shell: "{{ PGHOME }}/bin/pg_ctl stop -D {{ PGDATA }}"
          become_user: postgres
          when: not pg_service_exists.stat.exists

        - name: check that postgresql is stopped
          shell: '{{ PGHOME }}/bin/pg_ctl status -D {{ PGDATA }}'
          register: result
          until: result.stdout.find("no server running") != -1
          retries: 60
          delay: 1
          failed_when: result.rc != 3
          become_user: postgres

      when: migrate_host == 'replica'

    - name: migrate data by pg_upgrade on master
      block:

        - name: fix 44x version before update, add temp functions
          include_tasks: fix_44x_version_before_update.yml
          vars:
            _fvpbu_pghome: "{{ PGHOME }}"
            _fvpbu_pgdata: "{{ PGDATA }}"
            _fvpbu_add_tmp_function: true
          when: "( [ pg_current_version, '4.4.0' ] | compare_pg_se_versions )|int == 1 \
                  or ( [ pg_current_version, '4.4.1' ] | compare_pg_se_versions )|int == 1"

        - set_fact: pg_upgrade_time_started="{{ lookup('pipe','date \"+%Y-%m-%d %H:%M:%S\"') }}"

        - name: set pg_upgrade_log_link
          set_fact:
            pg_upgrade_log_link: "{{ local_backup_path }}/pg_upgrade_log_{{ ansible_date_time.date }}-T{{ ansible_date_time.hour }}{{ ansible_date_time.minute }}"

        - name: create pg_upgrade_temp_data dir
          file:
            path: "{{ item }}"
            state: directory
            mode: '0700'
            owner: 'postgres'
            group: 'postgres'
            recurse: yes
          with_items:
            - "{{ pg_upgrade_log_link }}"
            - "{{ pg_upgrade_log_link }}/pg_upgrade"

        - name: define current job count for pg_upgrade
          block:

            - name: get max_worker_process from new cfg
              reciter:
                ANSIBLE_MODULE_ARGS:
                  src: "{{ PGDATA }}/postgresql.conf"
                  action: get
                  parameter: max_worker_processes
              register: _updmajr_max_worker_processes
              environment:
                - PYTHONPATH: "{{ python.postgresql_venv_packages }}"

            - name: define free number of bgworkers
              set_fact:
                _updmajr_free_number_of_bgworkers: "{{ _updmajr_max_worker_processes.message|int - max_used_worker_process|int }}"

            - name: define current job count for pg_upgrade
              set_fact: 
                _updmajr_pg_upgrade_job_count: "{% if _updmajr_free_number_of_bgworkers|int >= proc_cpu_for_ansible_used|int %}\
                                                      {{ proc_cpu_for_ansible_used }}\
                                                {% else %}\
                                                      {{ _updmajr_free_number_of_bgworkers }}\
                                                {% endif %}"

        - name: migrate data by pg_upgrade with hard links key
          shell: cd {{ pg_upgrade_log_link }}/pg_upgrade; \
                 {{ PGHOME }}/bin/pg_upgrade \
                                           -b {{ PGHOME_OLD }}/bin/ \
                                           -B {{ PGHOME }}/bin/ \
                                           -d {{ PGDATA_OLD }} \
                                           -D {{ PGDATA }} \
                                           -p {{ ports.pg }} \
                                           -P 50433 \
                                           -j {{ _updmajr_pg_upgrade_job_count }} \
                                           --link \
                                           --retain
          environment:
            - LD_LIBRARY_PATH: "{{ PGHOME }}/lib"
            - PGHOST: 127.0.0.1
          ignore_errors: yes
          no_log: "{{ nolog }}"
          register: _updtmajor_migrate_result

        - set_fact: pg_upgrade_time_finished="{{ lookup('pipe','date \"+%Y-%m-%d %H:%M:%S\"') }}"
        - set_fact: pg_upgrade_time_work="{{ ((pg_upgrade_time_finished | to_datetime) - (pg_upgrade_time_started | to_datetime)).total_seconds() / 60 | int  }}"

        - debug: msg="{{ update_error_types_breakpoint_msg }}"
          when: is_recovery_test_mode and postgresql_error_um004m and migrate_host == 'master'

        - set_fact: pg_upgrade_print_log_time_started="{{ lookup('pipe','date \"+%Y-%m-%d %H:%M:%S\"') }}"

        - name: journal pg_upgrade
          include_tasks: pg_upgrade_log_print.yml
          vars:
            _pgupgrdlogprnt_log_dir: "{{ pg_upgrade_log_link }}/pg_upgrade"
          when: migrate_host == 'master' and is_print_pg_upgrade_logs

        - set_fact: pg_upgrade_print_log_time_finished="{{ lookup('pipe','date \"+%Y-%m-%d %H:%M:%S\"') }}"
        - set_fact: pg_upgrade_print_log_time_work="{{ ((pg_upgrade_print_log_time_finished | to_datetime) - (pg_upgrade_print_log_time_started | to_datetime)).total_seconds() / 60 | int  }}"

        - name: check result of data migrate
          assert:
            that: _updtmajor_migrate_result.stdout.find("Upgrade Complete") != -1
            fail_msg: "{{ update_control_msgs.fails.pg_upgrade_has_error  \
                        | replace('pg_upgrade_log_link',pg_upgrade_log_link + '/pg_upgrade') }}"
        
        - name: fix 44x version before update, drop temp functions
          include_tasks: fix_44x_version_before_update.yml
          vars:
            _fvpbu_pghome: "{{ PGHOME }}"
            _fvpbu_pgdata: "{{ PGDATA }}"
            _fvpbu_add_tmp_function: false
          when: "( [ pg_current_version, '4.4.0' ] | compare_pg_se_versions )|int == 1 \
                  or ( [ pg_current_version, '4.4.1' ] | compare_pg_se_versions )|int == 1"

      become_user: postgres
      when: migrate_host == 'master'

    - name: migrate data to replica by rsync from master
      block:

        - set_fact: rsync_time_started="{{ lookup('pipe','date \"+%Y-%m-%d %H:%M:%S\"') }}"

        - name: create temp ssh key to rsync
          openssh_keypair:
            path: "{{ PGBACKUP }}/ssh_key_rsync"
            state: present
          register: ssh_key_from_migrate
          delegate_to: replica
          no_log: "{{ nolog }}"

        - name: apply temp ssh key to master
          authorized_key:
            user: "{{ ansible_user }}"
            key: "{{ ssh_key_from_migrate.public_key }}"
            state: present
          no_log: "{{ nolog }}"

        - name: mirgate data to replica by rsync from master
          synchronize:
            mode: pull
            src: "{{ PGDATA.split('/')[:-1] | join('/') }}"
            dest: "/pgdata"
            rsync_path: "sudo rsync"
            delete: yes
            archive: yes
            perms: yes
            checksum: yes
            compress: "{% if is_compress_data_by_rsync is defined %}{{ is_compress_data_by_rsync }}{% else %}no{% endif %}"
            rsync_opts:
              - "--hard-links"
              - "--progress"
              - "--size-only"
              - "--no-inc-recursive"
              - "--rsh=ssh -S none -i {{ ssh_key_from_migrate.filename }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
          delegate_to: replica
          register: _updtmajor_rsync
          until: _updtmajor_rsync.rc|int == 0
          retries: 10
          delay: 10
          no_log: "{{ nolog }}"
          ignore_errors: true

        - name: remove temp ssh key to master
          authorized_key:
            user: "{{ ansible_user }}"
            key: "{{ ssh_key_from_migrate.public_key }}"
            state: absent
          no_log: "{{ nolog }}"

        - name: remove temp ssh key to rsync
          openssh_keypair:
            path: "{{ PGBACKUP }}/ssh_key_rsync"
            state: absent
          delegate_to: replica
          no_log: "{{ nolog }}"

        - set_fact: rsync_time_finished="{{ lookup('pipe','date \"+%Y-%m-%d %H:%M:%S\"') }}"
        - set_fact: rsync_time_work="{{ ((rsync_time_finished | to_datetime) - (rsync_time_started | to_datetime)).total_seconds() / 60 | int  }}"

        - debug: msg="{{ update_error_types_breakpoint_msg }}"
          when: is_recovery_test_mode and postgresql_error_um005r and migrate_host == 'replica'

        - set_fact: rsync_print_log_time_started="{{ lookup('pipe','date \"+%Y-%m-%d %H:%M:%S\"') }}"

        - name: write to json file rsync status info
          include_role:
            name: common
            tasks_from: write_to_json_file.yml
          vars:
            _wrttojson_dir: "{{ pg_upgrade_log_link }}/rsync"
            _wrttojson_filename: rsync
            _wrttojson_native_data: "{{ _updtmajor_rsync }}"
          
        - set_fact: rsync_print_log_time_finished="{{ lookup('pipe','date \"+%Y-%m-%d %H:%M:%S\"') }}"
        - set_fact: rsync_print_log_time_work="{{ ((rsync_print_log_time_finished | to_datetime) - (rsync_print_log_time_started | to_datetime)).total_seconds() / 60 | int  }}"

          # shell: |
          #   spawn /usr/bin/rsync -e "ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no" --archive --delete --hard-links --progress --size-only --no-inc-recursive {{ rsync_copy_data.src }} {{ ansible_user }}@{{ hostvars['replica'].ansible_fqdn }}:{{ rsync_copy_data.dst }}
          #   expect "password:"
          #   send  "{{ ansible_password | escape_string }}\r"
          #   expect eof
          # loop:
          #   - { src: "/pgdata/{{ pg_major_version }}" , dst: "/pgdata" }
          # loop_control:
          #   loop_var: rsync_copy_data
          # args:
          #   executable: expect
          # register: _updtmajor_rsync
          # until: _updtmajor_rsync.rc|int == 0
          # retries: 60
          # delay: 10
          # ignore_errors: true

        - name: stop migrate data to replica by rsync and print error
          fail: 
            msg: "error: rsync returned {{ _updtmajor_rsync.rc }} result code"
          when: _updtmajor_rsync.rc|int != 0

        # - name: calc checksums of {{ PGDATA }}
        #   calc_checksums_of_files:
        #     dir_path: "{{ PGDATA }}"
        #   delegate_to: "{{ item }}"
        #   loop:
        #     - "{{ hostvars['master'].ansible_fqdn }}"
        #     - "{{ hostvars['replica'].ansible_fqdn }}"
        #   register: pgdata_checksums      
        #   become_user: root
        #   when: inventory_hostname == 'master'

        # - name: check calculation checksums of {{ PGDATA }} directory after rsync
        #   assert:
        #     that: pgdata_checksums.results[0].message == pgdata_checksums.results[1].message
        #     fail_msg: "Result calculation checksums of {{ PGDATA }} directory after rsync is incorrect"
        #     success_msg: "Result is correct"
        #   when: inventory_hostname == 'master'

        # - name: calc checksums of {{ tablespace_location }}
        #   calc_checksums_of_files:
        #     dir_path: "{{ tablespace_location }}"
        #   delegate_to: "{{ item }}"
        #   loop:
        #     - "{{ hostvars['master'].ansible_fqdn }}"
        #     - "{{ hostvars['replica'].ansible_fqdn }}"
        #   register: pgdata_checksums      
        #   become_user: root
        #   when: inventory_hostname == 'master'

        # - name: check calculation checksums of {{ tablespace_location }} directory after rsync
        #   assert:
        #     that: pgdata_checksums.results[0].message == pgdata_checksums.results[1].message
        #     fail_msg: "Result calculation checksums of {{ tablespace_location }} directory after rsync is incorrect"
        #     success_msg: "Result is correct"
        #   when: inventory_hostname == 'master'

        # - name: stop migrate data to replica by rsync and print error
        #   fail: 
        #     msg: "error: checksums of tablespace_location directory on master and replica after rsync not equal"
        #   when: "pgdata_checksums_replica != pgdata_checksums_master"

        
      when: migrate_host == 'replica'

    - name: copy patroni.dynamic.json to PGDATA
      copy:
        src: "{{ PGDATA_OLD }}/patroni.dynamic.json"
        dest: "{{ PGDATA }}/patroni.dynamic.json"
        owner: postgres
        group: postgres
        remote_src: yes
        mode: 0600
      when: migrate_host == 'master' and is_patroni_exists

    - name: unset major_main_migrate_db error and set major_main_start_after_migrate_db error
      set_fact:
        update_errors: "{{ update_errors|combine(data, recursive=True) }}"
        cacheable: yes
      vars:
        data:
          types:
            pg: 
              major_main_migrate_master_db: false
              major_main_migrate_replica_db: false
              major_main_start_after_migrate_db: true
      run_once: true

    - name: remove 'trust' from postgresql.conf
      lineinfile:
        path: "{{ PGDATA }}/{{ item.cfg }}"
        regexp: "{{ item.line }}"
        state: absent
      with_items:
        - {cfg: "postgresql.conf", line: "enabled_extra_auth_methods = 'trust'"}
        - {cfg: "pg_hba.conf",     line: "local all postgres trust"}
      become_user: postgres
      when: migrate_host == 'master'

    - name: start new version Pangolin by postgresql.service
      systemd:
        name: postgresql
        state: started
        enabled: yes
        daemon_reload: yes
      become_user: root
      when: not is_patroni_exists

    - name: start patroni and pangolin
      service:
        name: patroni
        state: started
      become_user: root
      when: is_patroni_exists and migrate_host == 'replica'

    - name: turn off pause mode after patroni nodes updated
      include_role:
        name: patroni
        tasks_from: update_with_patronictl.yml
      vars:
        change_params: "pause: false"
      when: is_patroni_exists and migrate_host == 'replica'

    - name: start Pangolin by pg_ctl
      shell: "{{ PGHOME }}/bin/pg_ctl start -D {{ PGDATA }}"
      become_user: postgres
      when: is_patroni_exists and migrate_host == 'master'

    - name: loop wait for pgsql started
      shell: '{{ PGHOME }}/bin/pg_isready -h 127.0.0.1 -p {{ ports.pg }}'
      register: result
      until: result.stdout.find("accepting connections") != -1
      retries: 60
      delay: 1
      become_user: postgres

    - debug: msg="{{ update_error_types_breakpoint_msg }}"
      when: is_recovery_test_mode and postgresql_error_um006m and migrate_host == 'master'
    - debug: msg="{{ update_error_types_breakpoint_msg }}"
      when: is_recovery_test_mode and postgresql_error_um006r and migrate_host == 'replica'

    - name: make dump and calc dump checksums
      include_role:
        name: common
        tasks_from: make_dump_and_calc_dump_checksums.yml
      vars:
        _mdacdc_pghome: "{{ PGHOME }}"
        _mdacdc_pangolin_version: "{{ pg_version }}"
        _mdacdc_python_venv: "{{ python.postgresql_venv }}"
      when: "is_compare_checksums \
             and migrate_host == 'master' \
             and not is_recovery_test_mode"

    - name: send reload command to postgresql
      shell: "{{ PGHOME }}/bin/pg_ctl reload -D {{ PGDATA }}"
      become_user: postgres
      when: migrate_host == 'master'

    - name: check that checksums old and new db dump is equal
      assert:
        that: _mdacdc_checksums_old_dump[ index ].stdout == _mdacdc_checksums_new_dump[ index ].stdout
        fail_msg: "ERROR: checksums old and new db dump is NOT equal"
        success_msg: "OK: checksums old and new db dump is equal"
      loop: "{{_mdacdc_checksums_new_dump}}"
      loop_control:
        index_var: index
      when: "is_compare_checksums \
             and migrate_host == 'master' \
             and not is_recovery_test_mode"

    - name: update dir for {{ tablespace_location }}
      set_fact:
        tablespace_location: "/{{ tablespace_location.split('/').1 }}/{{ tablespace_location.split('/').2 }}/{{ tablespace_old_path.split('/')[-1] }}"
      when: "tablespace_location.split('/')[-1] != tablespace_old_path.split('/')[-1]"

    - name: update user tablespaces
      include_role:
        name: postgresql
        tasks_from: mv_user_tablespaces
      vars:
        _mvusertbl_pghome: "{{ PGHOME }}"
        _mvusertbl_pgdata: "{{ PGDATA }}"
      when: migrate_host == 'master'

    - debug: msg="{{ update_error_types_breakpoint_msg }}"
      when: is_recovery_test_mode and postgresql_error_um007m and migrate_host == 'master'

    - name: unset major_main_start_after_migrate_db error
      set_fact:
        update_errors: "{{ update_errors|combine(data, recursive=True) }}"
        cacheable: yes
      vars:
        data:
          types:
            pg: 
              major_main_start_after_migrate_db: false
      run_once: true

  rescue:

    - name: replica updated with error
      set_fact:
        update_errors: "{{ update_errors|combine(data, recursive=True) }}"
        cacheable: yes
      vars:
        data:
          aggregate: true
          hosts:
            replica: true
          components:
            pg: true
      run_once: true
      when: inventory_hostname == 'replica'

    - name: master updated with error
      set_fact:
        update_errors: "{{ update_errors|combine(data, recursive=True) }}"
        cacheable: yes
      vars:
        data:
          aggregate: true
          hosts:
            master: true
          components:
            pg: true
      run_once: true
      when: inventory_hostname == 'master'

  always:

    - name: set python interpretator
      set_fact:
        ansible_python_interpreter: '{{ python.global_bin_2 }}'

  become: true